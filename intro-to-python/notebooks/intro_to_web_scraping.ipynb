{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 101\n",
    "In this notebook, we are going to go through a simple web scraping example. For this tutorial,\n",
    "we will use a website which has publicly accessible data about a **10K** Marathon which \n",
    "happened in 2017 in Oregon, USA. Our target website is this [one](http://www.hubertiming.com/results/2017GPTR10K). On the website, there is data about each Marathon contestant such as how wel they did (place), their name, bib id and more. \n",
    "\n",
    "Our web scraping task is to extract all this data and save it as CSV file on our computer. We will use two main Python packages for this web scraping task: **request** and **BeatifulSoup**. These can be considered as the de facto tools for this task in Python. We will follow the follow hese steps:\n",
    "- **Quickly inspect the website**: Since its a straight foward website (the data is in HTML table), we will spend little time here.\n",
    "- **Open the website using request:** Here will check a few HTML tags (including table tag(tr)) and inspect their contents.\n",
    "- **Retrieve data from HTML table**: This will involve some clean tasks utilising Python built-in String methods\n",
    "- **Create a dataframe and save it to file:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Setup\n",
    "As usual, we import necessary packages/libraries. If you dont have the libraries, the first step would be to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open The Web URL Using Requests\n",
    "We use the ```get``` method to open the URL and then use the ```text``` \n",
    "on the response to retrieve the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"http://www.hubertiming.com/results/2017GPTR10K\"\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "# Lets check if its really an HTML\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use BeatifulSoup to Parse the HTML\n",
    "Getting the html of the page is just the first step. Next step is to create a Beautiful Soup object from the html. This is done by passing the html to the BeautifulSoup() function. The Beautiful Soup package is used to parse the html, that is, take the raw html text and break it into Python objects. The second argument ```lxml``` is the html parser whose details you do not need to worry about at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create BeatifulSoup Object\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Information Using the Soup Object\n",
    "The soup object allows you to extract interesting information about the website you're scraping such as getting the title of the page as shown below. For instance, we can get the title of the html. Also, you can get the text of the webpage and quickly print it out to check if it is what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>2017 Intel Great Place to Run 10K \\ Urban Clash Games Race Results</title>\n"
     ]
    }
   ],
   "source": [
    "# Get the title\n",
    "title = soup.title\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the text\n",
    "text = soup.get_text()\n",
    "\n",
    "# Note how this is different from the HTML we printed above\n",
    "print(soup.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Information Using HTML Tags\n",
    "Recall the following about HTML:\n",
    "- HTML elements are the building blocks of HTML pages\n",
    "- HTML elements are represented by tags\n",
    "- HTML tags label pieces of content such as \"heading\", \"paragraph\", \"table\", and so on\n",
    "- Browsers do not display the HTML tags, but use them to render the content of the page\n",
    "For a full reference of tags, see [this HTML reference](https://www.w3schools.com/tags/)\n",
    "\n",
    "We extract the information we need from the HTML using these tags, therefore its important to understand them.\n",
    "The **soup** object has many methods but we will use the ```find_all()``` method to extract useful html tags within a webpage such as ```<a>``` tag for hyperlinks.\n",
    "\n",
    "For our task here, the useful links are as below:\n",
    "- ```<table>``` for tables\n",
    "- ```<tr>``` for table rows\n",
    "- ```<th>``` for table headers\n",
    "- ``` <td>``` for table cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Just for fun, lets see how you can extract all the hyperlinks within the webpage in code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/results/2017GPTR\n",
      "https://www.hubertiming.com/\n",
      "#individual\n",
      "#team\n",
      "mailto:timing@hubertiming.com\n",
      "#tabs-1\n",
      "None\n",
      "None\n",
      "https://www.hubertiming.com/\n",
      "https://facebook.com/hubertiming/\n"
     ]
    }
   ],
   "source": [
    "# Extract all hyperlinks\n",
    "soup.find_all('a')\n",
    "\n",
    "# the find all method returns a list\n",
    "all_links = soup.find_all(\"a\")\n",
    "\n",
    "# Loop through the list to retrieve required attributes from the hyperlinks\n",
    "for link in all_links:\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output above, html tags sometimes come with attributes such as class, src, etc. These attributes provide additional information about html elements. You can use a for loop and the get('\"href\") method to extract and print out only hyperlinks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Tabular Information from HTML into a Python DataFrame\n",
    "Eventually what we would want to do is access a website, extract the information we need and then save it to disk or do some analysis with it. To get there, you should get all table rows in list form first and then convert that list into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tr><td>Finishers:</td><td>577</td></tr>, <tr><td>Male:</td><td>414</td></tr>, <tr><td>Female:</td><td>163</td></tr>, <tr class=\"header\">\n",
      "<th>Place</th>\n",
      "<th>Bib</th>\n",
      "<th>Name</th>\n",
      "<th>Gender</th>\n",
      "<th>City</th>\n",
      "<th>State</th>\n",
      "<th>Chip Time</th>\n",
      "<th>Chip Pace</th>\n",
      "<th>Gender Place</th>\n",
      "<th>Age Group</th>\n",
      "<th>Age Group Place</th>\n",
      "<th>Time to Start</th>\n",
      "<th>Gun Time</th>\n",
      "<th>Team</th>\n",
      "</tr>, <tr>\n",
      "<td>1</td>\n",
      "<td>814</td>\n",
      "<td>JARED WILSON</td>\n",
      "<td>M</td>\n",
      "<td>TIGARD</td>\n",
      "<td>OR</td>\n",
      "<td>00:36:21</td>\n",
      "<td>05:51</td>\n",
      "<td>1 of 414</td>\n",
      "<td>M 36-45</td>\n",
      "<td>1 of 152</td>\n",
      "<td>00:00:03</td>\n",
      "<td>00:36:24</td>\n",
      "<td></td>\n",
      "</tr>]\n"
     ]
    }
   ],
   "source": [
    "# Lets get all table rows and inspect the rows\n",
    "rows = soup.find_all('tr')  # A list of table rows\n",
    "print(rows[:5]) # print only first 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<td>14TH</td>, <td>INTEL TEAM M</td>, <td>04:43:23</td>, <td>00:58:59 - DANIELLE CASILLAS</td>, <td>01:02:06 - RAMYA MERUVA</td>, <td>01:17:06 - PALLAVI J SHINDE</td>, <td>01:25:11 - NALINI MURARI</td>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check a single row\n",
    "for row in rows:\n",
    "    row_td = row.find_all('td')\n",
    "\n",
    "print(row_td)\n",
    "type(row_td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows that each row is printed with html tags embedded in each row. This is not what we want. We can remove the html tags using Beautiful Soup or regular expressions (not recommended). The easiest way to remove html tags is to use Beautiful Soup, and it takes just one line of code to do this. Pass the string of interest into BeautifulSoup() and use the get_text() method to extract the text without html tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine further  the one table cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(row_td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14TH, INTEL TEAM M, 04:43:23, 00:58:59 - DANIELLE CASILLAS, 01:02:06 - RAMYA MERUVA, 01:17:06 - PALLAVI J SHINDE, 01:25:11 - NALINI MURARI]\n"
     ]
    }
   ],
   "source": [
    "# Convert table cell from bs4.element.ResultSet to String\n",
    "# This step os required because Soup object acceps Strings\n",
    "# When we want to get text from the HTML tags\n",
    "str_cells = str(row_td)\n",
    "\n",
    "# Now Lets get the text\n",
    "clean_text = BeautifulSoup(str_cells, \"lxml\").get_text()\n",
    "\n",
    "# And Lets see the result\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Pandas DataFrame From The Website Data\n",
    "Now that we know how to extract the text only from HTML tags,\n",
    "we will loop through all rows, get cell contents, \n",
    "use BS to extract text without HTML tags from the cell, \n",
    "do some string cleaning and finally put that row into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def clean_table_row(row=None):\n",
    "    # Extract cell using the cell HTML tag\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    # Convert to String\n",
    "    str_cells = str(cells)\n",
    "    \n",
    "    # Use BS to extract only text and remove HTML tags\n",
    "    cleantext = BeautifulSoup(str_cells, \"lxml\").get_text()\n",
    "    \n",
    "    # Note that we have brackets at each end, we remove them using list indexing\n",
    "    cleantext2 = cleantext[1:-1]\n",
    "    \n",
    "    # This string: cleantext2 is just one whole string, so we split using the delimiter (comma)\n",
    "    split_str = cleantext2.split(',') # returns a list\n",
    "    \n",
    "    return split_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Through All Rows, Clean Them and Put in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will collect our cleaned rows in this list\n",
    "cleaned_rows = []\n",
    "\n",
    "for row in rows:\n",
    "    # Clean up the row using our little function above\n",
    "    clean_row = clean_table_row(row)\n",
    "    \n",
    "    # We are only interested in a full table row, \n",
    "    # so discard the rest of the rows using length\n",
    "    if len(clean_row) > 10:\n",
    "        cleaned_rows.append(clean_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " ' 814',\n",
       " ' JARED WILSON',\n",
       " ' M',\n",
       " ' TIGARD',\n",
       " ' OR',\n",
       " ' 00:36:21',\n",
       " ' 05:51',\n",
       " ' 1 of 414',\n",
       " ' M 36-45',\n",
       " ' 1 of 152',\n",
       " ' 00:00:03',\n",
       " ' 00:36:24',\n",
       " ' ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_rows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Table Headers\n",
    "We use the same process as above to clean up the headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Place', 'Bib', 'Name', 'Gender', 'City', 'State', 'Chip Time', 'Chip Pace', 'Gender Place', 'Age Group', 'Age Group Place', 'Time to Start', 'Gun Time', 'Team']\n"
     ]
    }
   ],
   "source": [
    "# Get Table headers using 'th' HTML tag\n",
    "headers_with_tags = soup.find_all('th')\n",
    "\n",
    "# Convert to string\n",
    "headers_str = str(headers_with_tags)\n",
    "\n",
    "# Extract text only and leave out HTML tags\n",
    "headers_without_tags = BeautifulSoup(headers_str, \"lxml\").get_text()\n",
    "headers_without_tags2 = headers_without_tags[1:-1]\n",
    "\n",
    "# Split using comma delimeter and remove any trailing spaces\n",
    "split_header = headers_without_tags2.split(',')\n",
    "split_header2 = [i.strip() for i in split_header] \n",
    "\n",
    "# Lets check out the headers now\n",
    "print(split_header2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, Lets Create The DataFrame\n",
    "We have a nested list: *cleaned_rows* where each element is a list containing a cells of a single table row. \n",
    "We also have column headers. We use these two to create a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Place</th>\n",
       "      <th>Bib</th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Chip Time</th>\n",
       "      <th>Chip Pace</th>\n",
       "      <th>Gender Place</th>\n",
       "      <th>Age Group</th>\n",
       "      <th>Age Group Place</th>\n",
       "      <th>Time to Start</th>\n",
       "      <th>Gun Time</th>\n",
       "      <th>Team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>814</td>\n",
       "      <td>JARED WILSON</td>\n",
       "      <td>M</td>\n",
       "      <td>TIGARD</td>\n",
       "      <td>OR</td>\n",
       "      <td>00:36:21</td>\n",
       "      <td>05:51</td>\n",
       "      <td>1 of 414</td>\n",
       "      <td>M 36-45</td>\n",
       "      <td>1 of 152</td>\n",
       "      <td>00:00:03</td>\n",
       "      <td>00:36:24</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>573</td>\n",
       "      <td>NATHAN A SUSTERSIC</td>\n",
       "      <td>M</td>\n",
       "      <td>PORTLAND</td>\n",
       "      <td>OR</td>\n",
       "      <td>00:36:42</td>\n",
       "      <td>05:55</td>\n",
       "      <td>2 of 414</td>\n",
       "      <td>M 26-35</td>\n",
       "      <td>1 of 154</td>\n",
       "      <td>00:00:03</td>\n",
       "      <td>00:36:45</td>\n",
       "      <td>INTEL TEAM F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>687</td>\n",
       "      <td>FRANCISCO MAYA</td>\n",
       "      <td>M</td>\n",
       "      <td>PORTLAND</td>\n",
       "      <td>OR</td>\n",
       "      <td>00:37:44</td>\n",
       "      <td>06:05</td>\n",
       "      <td>3 of 414</td>\n",
       "      <td>M 46-55</td>\n",
       "      <td>1 of 64</td>\n",
       "      <td>00:00:04</td>\n",
       "      <td>00:37:48</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>623</td>\n",
       "      <td>PAUL MORROW</td>\n",
       "      <td>M</td>\n",
       "      <td>BEAVERTON</td>\n",
       "      <td>OR</td>\n",
       "      <td>00:38:34</td>\n",
       "      <td>06:13</td>\n",
       "      <td>4 of 414</td>\n",
       "      <td>M 36-45</td>\n",
       "      <td>2 of 152</td>\n",
       "      <td>00:00:03</td>\n",
       "      <td>00:38:37</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>569</td>\n",
       "      <td>DEREK G OSBORNE</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>00:39:21</td>\n",
       "      <td>06:20</td>\n",
       "      <td>5 of 414</td>\n",
       "      <td>M 26-35</td>\n",
       "      <td>2 of 154</td>\n",
       "      <td>00:00:03</td>\n",
       "      <td>00:39:24</td>\n",
       "      <td>INTEL TEAM F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Place   Bib                 Name Gender        City State  Chip Time  \\\n",
       "0     1   814         JARED WILSON      M      TIGARD    OR   00:36:21   \n",
       "1     2   573   NATHAN A SUSTERSIC      M    PORTLAND    OR   00:36:42   \n",
       "2     3   687       FRANCISCO MAYA      M    PORTLAND    OR   00:37:44   \n",
       "3     4   623          PAUL MORROW      M   BEAVERTON    OR   00:38:34   \n",
       "4     5   569      DEREK G OSBORNE      M   HILLSBORO    OR   00:39:21   \n",
       "\n",
       "  Chip Pace Gender Place Age Group Age Group Place Time to Start   Gun Time  \\\n",
       "0     05:51     1 of 414   M 36-45        1 of 152      00:00:03   00:36:24   \n",
       "1     05:55     2 of 414   M 26-35        1 of 154      00:00:03   00:36:45   \n",
       "2     06:05     3 of 414   M 46-55         1 of 64      00:00:04   00:37:48   \n",
       "3     06:13     4 of 414   M 36-45        2 of 152      00:00:03   00:38:37   \n",
       "4     06:20     5 of 414   M 26-35        2 of 154      00:00:03   00:39:24   \n",
       "\n",
       "            Team  \n",
       "0                 \n",
       "1   INTEL TEAM F  \n",
       "2                 \n",
       "3                 \n",
       "4   INTEL TEAM F  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Pandas DataFrame\n",
    "df = pd.DataFrame(data=cleaned_rows, columns=split_header2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save The DataFrame\n",
    "Once we are happy with our dataframe, we can save it as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save AS CSV into data folder\n",
    "out_file = '../data/marathoners.csv'\n",
    "\n",
    "# The index = False option ensures we dont save the default index\n",
    "df.to_csv(out_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE\n",
    "In order to make the exercises easy and fast, I looked for a similar website for you to scrape. The [target](https://www.tcsnycmarathon.org/about-the-race/results/overall-men) website also contains Marathon results for men only. The task is the same as in the example, there is a table of Marathon results, please extract the data and save it into\n",
    "a CSV file. A bonus task: for those who finish fast, please tabulate number of participants by country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the website using requests, retrieve HTML and create BS object\n",
    "url = YOUR CODE\n",
    "r = YOUR CODE\n",
    "html = YOUR CODE\n",
    "bs = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract All Rows and Inspect Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tr bgcolor=\"#FFFFFF\"><td align=\"center\">\n",
      "\t\t\t\t1</td>\n",
      "<td align=\"center\">\n",
      "\t\t\t\t3</td>\n",
      "<td nowrap=\"nowrap\">\n",
      "\t\t\t\tLelisa Desisa</td>\n",
      "<td align=\"center\">\n",
      "\t\t\t\t2:05:59</td>\n",
      "<td align=\"center\">\n",
      "\t\t\t\t </td>\n",
      "<td nowrap=\"nowrap\">\n",
      "\t\t\t\tEthiopia</td>\n",
      "<td align=\"center\">\n",
      "\t\t\t\tETH</td>\n",
      "</tr>\n"
     ]
    }
   ],
   "source": [
    "# Extract all table rows using the tr HTML tag and inspect the first couple of rows\n",
    "all_rows = YOUR CODE  # A list of table rows\n",
    "print(all_rows[1]) # print only first 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Through Rows, Clean Table Cells and Save Into a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare list to hold all cleaned rows\n",
    "cleaned_rows = [] \n",
    "\n",
    "for row in all_rows:\n",
    "    # Extract cell using table cell HTML tag\n",
    "    cells = YOUR CODE\n",
    "    \n",
    "    # Extract text only\n",
    "    str_cells = str(cells)\n",
    "    clean_text = BeautifulSoup(str_cells, \"lxml\").get_text()\n",
    "    \n",
    "    # Remove white spaces-a little convuluted but it works\n",
    "    clean_text2 = \" \".join(clean_text.split())\n",
    "\n",
    "    # Remove brackts at beginning and end\n",
    "    clean_text3 = clean_text2[1:-1]\n",
    "    \n",
    "    # Split clean_text3 using comma delimiter\n",
    "    YOUR CODE\n",
    "    \n",
    "    # Remove white spaces again\n",
    "    split_str2 = [i.strip() for i in split_str]\n",
    "    \n",
    "    # Add split_str2 to cleaned_rows list\n",
    "    YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A DataFrame and Inspect It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names: note that the first element of the list contains the column names\n",
    "# Use list indexing to get the column headers\n",
    "colnames = YOUR CODE\n",
    "\n",
    "# Create Dataframe\n",
    "df_men = pd.DataFrame(data=cleaned_rows[1:], columns=YOUR CODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Dataframe\n",
    "YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the DataFrame to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into the data folder in the ch1 workig directory\n",
    "output_filename = YOUR CODE\n",
    "\n",
    "# save to CSV file\n",
    "YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Check Number of Men Marathoners by Country\n",
    "Hint: Use pandas DataFrame ```value_counts()``` method for this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
